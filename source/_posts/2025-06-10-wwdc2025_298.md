---
title: 借助 MLX 在 Apple 芯片上探索大语言模型
date: 2025-06-10 15:45:43
categories:
- wwdc2025
tags:
- wwdc2025
- ios
- ipados
- macos
- 机器学习与-ai
---
了解 mlx lm，这款工具专为在 apple 芯片上轻松高效地处理大语言模型而设计。我们将介绍如何在 mac 上微调先进的大语言模型并以此运行推理，以及如何将这些模型无缝整合到基于 swift 的应用程序和项目中。
<!--more-->

![视频封面](https://devimages-cdn.apple.com/wwdc-services/images/3055294D-836B-4513-B7B0-0BC5666246B0/10061/10061_wide_250x141_2x.jpg)
[视频地址](https://developer.apple.com/cn/videos/play/wwdc2025/298/)

# 利用 MLX 在 Apple 芯片上高效运行大语言模型

## 引言

随着大语言模型(LLM)的快速发展，如何在本地设备上高效运行这些模型成为开发者关注的重点。Apple 芯片凭借其强大的性能和统一内存架构，为本地部署大语言模型提供了理想平台。本文将介绍专为 Apple 芯片优化的 MLX 框架及其 MLX LM 工具集，帮助开发者轻松实现大语言模型在 Mac 设备上的推理和微调。

## MLX 框架概述

MLX 是一个专为 Apple 芯片机器学习打造的开源库，具有以下核心特性：

- **Metal 加速**：充分利用 Apple 芯片的 GPU 性能
- **统一内存架构**：CPU 和 GPU 可以直接访问相同数据，无需数据拷贝
- **多语言支持**：提供 Python、Swift、C++ 和 C 等多种 API
- **高效推理**：支持数百亿参数模型的流畅交互

## 安装 MLX LM

使用 pip 即可轻松安装 MLX LM 工具集：

```
pip install mlx-lm
```

## 文本生成实践

### 命令行方式

最简单的文本生成方式是通过命令行：

```
mlx_lm.generate --model "mlx-community/Mistral-7B-Instruct-v0.3-4bit" \
                --prompt "用 Swift 实现快速排序"
```

### Python API

更灵活的生成方式是通过 Python API：

```python
from mlx_lm import load, generate

model, tokenizer = load("mlx-community/Mistral-7B-Instruct-v0.3-4bit")
prompt = tokenizer.apply_chat_template([{"role":"user","content":"Swift快速排序"}], True)
text = generate(model, tokenizer, prompt=prompt, verbose=True)
```

## 模型量化技术

量化是减小模型体积、提升推理速度的有效方法：

```
mlx_lm.convert --hf-path "mistralai/Mistral-7B-Instruct-v0.3" \
               --mlx-path "./mistral-7b-v0.3-4bit" \
               --quantize --q-bits 4 --q-group-size 64
```

还可以实现混合精度量化：

```python
def mixed_quantization(layer_path, layer, model_config):
    if "lm_head" in layer_path: return {"bits":6, "group_size":64}
    return {"bits":4, "group_size":64} if hasattr(layer,"to_quantized") else False

convert(hf_path="mistralai/Mistral-7B-v0.3", quant_predicate=mixed_quantization)
```

## 模型微调方法

MLX LM 支持两种微调模式：

1. 全参数微调（资源需求较高）
2. 低秩适配器(LoRA)训练（仅训练新增参数）

### LoRA 训练示例

```
mlx_lm.lora --model "mlx-community/Mistral-7B-Instruct-v0.3-4bit" \
            --train --data ./data --iters 300
```

### 适配器融合

训练完成后可以将适配器融合回原模型：

```
mlx_lm.fuse --model "./mistral-7b-v0.3-4bit" \
            --adapter_path "adapters" \
            --save_path "fused-mistral-7b"
```

## Swift 集成

MLX 提供了简洁的 Swift API，28 行代码即可实现完整推理流程：

```swift
import MLXLLM

let model = try await LLMModelFactory.shared.loadContainer(configuration: .init(id:"mlx-community/Mistral-7B-4bit"))
try await model.perform { context in
    let input = try await context.processor.prepare(input: .init(prompt:"Swift快速排序"))
    for await part in generate(input: input, context: context) {
        print(part.chunk ?? "", terminator:"")
    }
}
```

添加 KV 缓存支持仅需额外一行：

```swift
let cache = context.model.newCache(parameters: .init())
let tokenIter = try TokenIterator(input: input, model: context.model, cache: cache)
```

## 结论

MLX 框架为 Apple 芯片上的大语言模型应用提供了完整的解决方案，从模型加载、量化、微调到 Swift 应用集成，覆盖了开发全流程。借助 MLX 的高效实现，开发者可以在 Mac 设备上运行数十亿参数的大模型，为本地 AI 应用开发开辟了新的可能性。

## 相关视频

[开始使用适用于 Apple 芯片的 MLX](https://developer.apple.com/videos/play/wwdc2025/315)

## 文档资源

[MLX](https://ml-explore.github.io/mlx/)  
[MLX LM - Python API](https://github.com/ml-explore/mlx-lm)  
[MLX Explore - Python API](https://github.com/ml-explore/mlx)  
[MLX Framework](https://mlx-framework.org)  
[MLX Llama Inference](https://ml-explore.github.io/mlx/build/html/examples/llama-inference.html)  
[MLX Swift](https://github.com/ml-explore/mlx-swift)
> 此文章由AI生成，可能存在错误，如有问题，请联系[djs66256@163.com](djs66256@163.com)