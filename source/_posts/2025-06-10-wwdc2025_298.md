---
title: 借助 MLX 在 Apple 芯片上探索大语言模型
date: 2025-06-10 21:46:48
categories:
- wwdc2025
tags:
- wwdc2025
- ios
- ipados
- macos
- 机器学习与-ai
---
了解 mlx lm，这款工具专为在 apple 芯片上轻松高效地处理大语言模型而设计。我们将介绍如何在 mac 上微调先进的大语言模型并以此运行推理，以及如何将这些模型无缝整合到基于 swift 的应用程序和项目中。
<!--more-->

![视频封面](https://devimages-cdn.apple.com/wwdc-services/images/3055294D-836B-4513-B7B0-0BC5666246B0/10061/10061_wide_250x141_2x.jpg)
[视频地址](https://developer.apple.com/cn/videos/play/wwdc2025/298/)
> 此文章由AI生成，可能存在错误，如有问题，请联系[djs66256@163.com](djs66256@163.com)

# 借助 MLX 在 Apple 芯片上探索大语言模型

在 WWDC 2025 的技术分享中，MLX 团队工程师 Angelos 详细介绍了如何利用 MLX 工具链在 Apple 芯片设备上高效运行和微调大语言模型。这一解决方案为开发者提供了从命令行工具到 Python/Swift API 的完整工作流，使得本地化部署大语言模型变得前所未有地简单。

## MLX LM 工具链概览

MLX 是一款专为 Apple Silicon 机器学习设计的开源框架，其核心优势在于充分利用了 Metal 框架的 GPU 加速能力以及 Apple 芯片的统一内存架构。这种架构允许 CPU 和 GPU 无缝共享相同数据，消除了传统异构计算中的数据迁移开销。

MLX LM 是构建在 MLX 之上的 Python 工具包，专门针对大语言模型的推理和训练进行了优化。该工具链提供三大核心功能：

1. 命令行工具：无需编写代码即可完成模型推理和微调
2. Python API：提供完整的编程接口用于定制化模型操作
3. Swift 集成：原生支持在 Swift 应用中部署大语言模型

## 模型推理实战

MLX LM 提供了极其简单的模型加载方式。开发者可以通过 Hugging Face 社区直接获取数千个预训练模型，也可以通过本地路径加载自定义模型。以下命令展示了如何加载一个 4-bit 量化版的 Mistral-7B 模型：

```
mlx_lm.generate --model "mlx-community/Mistral-7B-Instruct-v0.3-4bit" \
                --prompt "Write a quick sort in Swift"
```

该工具会自动处理模型下载、提示词预处理和文本生成全流程。对于需要精细控制的场景，MLX LM 提供了完整的 Python API：

```
from mlx_lm import load, generate
model, tokenizer = load("mlx-community/Mistral-7B-Instruct-v0.3-4bit")
prompt = "Write a quick sort in Swift"
messages = [{"role": "user", "content": prompt}]
prompt = tokenizer.apply_chat_template(messages, add_generation_prompt=True)
text = generate(model, tokenizer, prompt=prompt, verbose=True)
```

## 高级特性：KV Cache 与量化

对于需要维持对话上下文的场景，MLX LM 提供了键值缓存（KV Cache）机制。该技术通过存储历史计算的注意力键值对，显著提升了长文本生成的效率：

```
from mlx_lm.models.cache import make_prompt_cache
cache = make_prompt_cache(model)
text = generate(model, tokenizer, prompt=prompt, prompt_cache=cache, verbose=True)
```

量化技术是另一个核心优势。MLX LM 内置的量化功能可以将模型压缩至 4-bit 甚至更低精度，同时保持可接受的精度损失。以下命令展示了如何将原始模型转换为 4-bit 量化版本：

```
mlx_lm.convert --hf-path "mistralai/Mistral-7B-Instruct-v0.3" \
               --mlx-path "./mistral-7b-v0.3-4bit" \
               --dtype float16 \
               --quantize --q-bits 4 --q-group-size 64
```

## 模型微调能力

MLX LM 支持两种微调模式：全参数微调和低秩适配器（LoRA）微调。后者特别适合在消费级硬件上操作，因为它仅需训练少量新增参数：

```
mlx_lm.lora --model "mlx-community/Mistral-7B-Instruct-v0.3-4bit" 
            --train 
            --data /path/to/our/data/folder
            --iters 300 
            --batch-size 16
```

微调完成后，开发者可以将适配器融合回基础模型，创建独立的可部署版本：

```
mlx_lm.fuse --model "./mistral-7b-v0.3-4bit" \
            --adapter-path "adapters" \
            --save-path "fused-mistral-7b-v0.3-4bit"
```

## Swift 原生支持

MLX 的 Swift API 使得在原生应用中集成大语言模型变得异常简单。以下代码展示了如何在 Swift 应用中加载模型并生成文本：

```
import MLX
import MLXLLM

let modelId = "mlx-community/Mistral-7B-Instruct-v0.3-4bit"
let modelFactory = LLMModelModelFactory.shared
let configuration = ModelConfiguration(id: modelId)
let model = try await modelFactory.loadContainer(configuration: configuration)

let prompt = "Write a quicksort in Swift"
let input = try await context.processor.prepare(input: UserInput(prompt: prompt))
let tokenStream = try generate(input: input, parameters: params, context: context)
```

## 生态资源与未来发展

MLX 项目完全开源，开发者可以通过以下资源深入了解：

相关视频：
[开始使用适用于 Apple 芯片的 MLX](https://developer.apple.com/videos/play/wwdc2025/315)

文档资源：
[MLX](https://ml-explore.github.io/mlx/)
[MLX LM - Python API](https://github.com/ml-explore/mlx-lm)
[MLX Explore - Python API](https://github.com/ml-explore/mlx)
[MLX Framework](https://mlx-framework.org)
[MLX Llama Inference](https://ml-explore.github.io/mlx/build/html/examples/llama-inference.html)
[MLX Swift](https://github.com/ml-explore/mlx-swift)

随着 Apple 芯片性能的持续提升和 MLX 生态的不断完善，在本地设备上部署和微调大语言模型正逐渐成为主流开发范式。这一技术方向不仅为隐私敏感应用提供了新可能，也为个性化AI体验开辟了道路。
